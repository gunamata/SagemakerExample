{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Deploy Your First Machine Learning Model on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker session\n",
    "\n",
    "A SageMaker session needs to be initialized in order to start interacting the SageMaker service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker as sage\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boto_session = boto3.Session(profile_name=\"up-sagemaker\")\n",
    "session = sage.Session(boto_session=boto_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training.\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "def _save_model(clf):\n",
    "    with open(os.path.join(\"model/\", 'model.pkl'), 'wb') as out:\n",
    "        pickle.dump(clf, out)\n",
    "        \n",
    "def train():\n",
    "    print('Starting the training.')\n",
    "    try:\n",
    "        data_df = pd.read_csv(\"data/iris.csv\", header=None)\n",
    "\n",
    "        # labels are in the first column\n",
    "        labels = data_df.iloc[:, 0]\n",
    "        features = data_df.iloc[:, 1:]\n",
    "\n",
    "        train_features, test_features, labels_train, labels_test = \\\n",
    "            train_test_split(\n",
    "                features, labels, test_size=0.3, random_state=42\n",
    "            )\n",
    "\n",
    "        max_leaf_nodes = 100\n",
    "\n",
    "        clf = ensemble.RandomForestClassifier(\n",
    "            max_leaf_nodes=max_leaf_nodes\n",
    "        )\n",
    "        clf = clf.fit(train_features, labels_train)\n",
    "\n",
    "        test_predictions = clf.predict(test_features)\n",
    "\n",
    "        precision = precision_score(\n",
    "            labels_test, test_predictions, average='macro'\n",
    "        )\n",
    "        recall = recall_score(\n",
    "            labels_test, test_predictions, average='macro'\n",
    "        )\n",
    "\n",
    "        clf = clf.fit(features, labels)\n",
    "        _save_model(clf)\n",
    "\n",
    "        print('Training complete.')\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            'An Exception during training: ' + str(e) + '\\n'\n",
    "        )\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the model to S3 bucket \n",
    "\n",
    "Using the SageMaker session, which was initialized earlier, the model will be upload to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_directory = 'model'\n",
    "s3_prefix = 'up-lambda-iris-model'\n",
    "\n",
    "model_location = session.upload_data(local_model_directory, key_prefix=s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an estimator and fit the model\n",
    "\n",
    "In order to use SageMaker to fit our algorithm, we'll create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n",
    "\n",
    "* The __container name__. This is constructed as in the shell commands above.\n",
    "* The __role__. As defined above.\n",
    "* The __instance count__ which is the number of machines to use for training.\n",
    "* The __instance type__ which is the type of machine to use for training.\n",
    "* The __output path__ determines where the model artifact will be written.\n",
    "* The __session__ is the SageMaker session object that we defined above.\n",
    "\n",
    "Then we use fit() on the estimator to train against the data that we uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-23 17:48:42 Starting - Starting the training job...\n",
      "2020-03-23 17:48:43 Starting - Launching requested ML instances......\n",
      "2020-03-23 17:49:47 Starting - Preparing the instances for training...\n",
      "2020-03-23 17:50:36 Downloading - Downloading input data\n",
      "2020-03-23 17:50:36 Training - Downloading the training image...\n",
      "2020-03-23 17:51:18 Uploading - Uploading generated training model\u001b[34mStarting the training.\u001b[0m\n",
      "\u001b[34mTraining complete.\u001b[0m\n",
      "\n",
      "2020-03-23 17:51:24 Completed - Training job completed\n",
      "Training seconds: 54\n",
      "Billable seconds: 54\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "account = session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = session.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/first-ml-model:latest'.format(account, region)\n",
    "\n",
    "role = get_execution_role(session)\n",
    "estimator = sage.estimator.Estimator(\n",
    "                        image,\n",
    "                        role, \n",
    "                        1, \n",
    "                        'ml.m4.xlarge',\n",
    "                        output_path=\"s3://{}/output\".format(session.default_bucket()),\n",
    "                        sagemaker_session=session\n",
    ")\n",
    "\n",
    "estimator.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "Using the trained `Estimator`, a RESTful service will be initialized on Amazon SageMaker service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer\n",
    "\n",
    "predictor = estimator.deploy(1, 'ml.m4.xlarge', serializer=json_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample some data and use it for a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feat_1': [5.8, 5.4, 5.0, 6.4, 5.7, 5.8, 5.4, 4.8, 5.0, 5.5],\n",
       " 'feat_2': [2.7, 3.9, 3.4, 3.2, 2.5, 2.6, 3.9, 3.1, 3.0, 4.2],\n",
       " 'feat_3': [3.9, 1.7, 1.6, 5.3, 5.0, 4.0, 1.3, 1.6, 1.6, 1.4],\n",
       " 'feat_4': [1.2, 0.4, 0.4, 2.3, 2.0, 1.2, 0.4, 0.2, 0.2, 0.2]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"data/iris.csv\", header=None, names=[\"feat_1\", \"feat_2\", \"feat_3\", \"feat_4\"])\n",
    "\n",
    "sampled_df = data_df.sample(10)\n",
    "\n",
    "sampled_post_request_body = sampled_df.to_dict(orient='list')\n",
    "\n",
    "sampled_post_request_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [\"versicolor\", \"setosa\", \"setosa\", \"virginica\", \"virginica\", \"versicolor\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"]}\n"
     ]
    }
   ],
   "source": [
    "print(predictor.predict(sampled_post_request_body).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
